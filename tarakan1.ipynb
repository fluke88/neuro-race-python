{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "from gym import spaces\n",
    "import math\n",
    "from pygame.math import Vector2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pygame\n",
    "\n",
    "def distance(x1, y1, x2, y2):\n",
    "    dx = x2 - x1\n",
    "    dy = y2 - y1\n",
    "    distance = math.sqrt(dx**2 + dy**2)\n",
    "    return distance\n",
    "\n",
    "def intersection(x1, y1, x2, y2, x3, y3, x4, y4):\n",
    "    line1_start = Vector2(x1, y1)\n",
    "    line1_end = Vector2(x2, y2)\n",
    "    line2_start = Vector2(x3, y3)\n",
    "    line2_end = Vector2(x4, y4)\n",
    "\n",
    "    line1_direction = line1_end - line1_start\n",
    "    line2_direction = line2_end - line2_start\n",
    "\n",
    "    if line1_direction.cross(line2_direction) != 0:\n",
    "        intersection_point = line1_start + line1_direction * ((line2_start - line1_start).cross(line2_direction) / line1_direction.cross(line2_direction))\n",
    "        \n",
    "        if (min(x1, x2) <= intersection_point.x <= max(x1, x2) and\n",
    "            min(y1, y2) <= intersection_point.y <= max(y1, y2) and\n",
    "            min(x3, x4) <= intersection_point.x <= max(x3, x4) and\n",
    "            min(y3, y4) <= intersection_point.y <= max(y3, y4)):\n",
    "            return intersection_point\n",
    "            \n",
    "    return None\n",
    "\n",
    "def find_intersection_points(barrier_array, car_x, car_y, end_x, end_y):\n",
    "    intersection_points = []\n",
    "    for i, barrier in enumerate(barrier_array):\n",
    "        intersection_point = intersection(barrier[0], barrier[1], barrier[2], barrier[3], car_x, car_y, end_x, end_y)\n",
    "        if intersection_point:\n",
    "            intersection_points.append(intersection_point)\n",
    "    return intersection_points\n",
    "\n",
    "def find_distance_to_intersection(intersection_points, car_x, car_y):\n",
    "    closest_distance = float('inf')\n",
    "    closest_intersection = None\n",
    "\n",
    "    for point in intersection_points:\n",
    "        x1, y1 = point\n",
    "        \n",
    "        dist = math.sqrt((x1 - car_x)**2 + (y1 - car_y)**2)\n",
    "\n",
    "        if dist < closest_distance:\n",
    "            closest_distance = dist\n",
    "            closest_intersection = point\n",
    "\n",
    "    return closest_distance, closest_intersection\n",
    "\n",
    "class MyCarEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.action_space = spaces.Discrete(3)        \n",
    "\n",
    "        self.WHITE = (255, 255, 255)\n",
    "        self.BLACK = (0, 0, 0)\n",
    "        self.GREEN = (0, 255, 0)\n",
    "        self.RED = (255, 0, 0)\n",
    "\n",
    "        self.barrier_array = np.array([[203, 270, 291, 151], [291, 151, 485, 181], [485, 181, 535, 349], [535, 349, 612, 367], [612, 367, 762, 226], \n",
    "                                               [762, 226, 962, 280], [962, 280, 980, 469], [980, 469, 934, 564], [934, 564, 739, 647], [739, 647, 641, 612],\n",
    "                                                 [641, 612, 598, 455], [598, 455, 515, 432], [515, 432, 403, 528], [403, 528, 240, 501], [240, 501, 186, 390], \n",
    "                                                 [186, 390, 205, 264], [327, 238, 279, 335], [279, 335, 297, 405], [297, 405, 391, 424], [391, 424, 449, 368], \n",
    "                                                 [449, 368, 428, 274], [428, 274, 326, 239], [670, 421, 716, 550], [716, 550, 848, 504], [848, 504, 885, 438], \n",
    "                                                 [885, 438, 879, 350], [879, 350, 784, 312], [784, 312, 670, 421]])\n",
    "\n",
    "        self.reward_lines_array = np.array([[428, 274, 486, 181], [328, 239, 290, 153], [278, 333, 203, 267], [280, 337, 186, 391], [296, 406, 239, 500], \n",
    "                                            [390, 426, 402, 527], [449, 367, 517, 433], [449, 368, 537, 348], [599, 456, 669, 421], [669, 421, 611, 367], \n",
    "                                            [784, 312, 762, 228], [879, 350, 962, 280], [886, 438, 982, 471], [846, 506, 933, 563], [716, 549, 739, 645], [717, 550, 640, 611]])\n",
    "\n",
    "        self.start_x = 560\n",
    "        self.start_y = 404\n",
    "        self.start_angle = 190\n",
    "\n",
    "        self.car_x = self.start_x \n",
    "        self.car_y = self.start_y\n",
    "        self.car_angle = self.start_angle\n",
    "        self.speed = 15\n",
    "        self.rotation_speed = 10\n",
    "\n",
    "        self.num_rays = 12\n",
    "        self.ray_length = 1000\n",
    "\n",
    "        \n",
    "        self.WINDOW_WIDTH, self.WINDOW_HEIGHT = 1268, 840\n",
    "\n",
    "        self.car_color = self.BLACK\n",
    "        self.car_width = 20\n",
    "        self.car_height = 20\n",
    "\n",
    "        self.state = np.ones(self.num_rays, dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(self.num_rays,))\n",
    "\n",
    "        self.reward = 0\n",
    "        self.car_surface = pygame.Surface((self.car_width, self.car_height), pygame.SRCALPHA)\n",
    "        \n",
    "        self.distance_array_for_fit = np.ones((1, self.num_rays), dtype=np.float32)\n",
    "\n",
    "        self.reward_last = []\n",
    "\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "\n",
    "        self.car_x = self.start_x \n",
    "        self.car_y = self.start_y\n",
    "        self.car_angle = self.start_angle\n",
    "        self.num_steps = 0\n",
    "        self.reward = 0\n",
    "        self.reward_last = []\n",
    "        self.reward_lines_array = np.array([[428, 274, 486, 181], [328, 239, 290, 153], [278, 333, 203, 267], [280, 337, 186, 391], [296, 406, 239, 500], \n",
    "                                [390, 426, 402, 527], [449, 367, 517, 433], [449, 368, 537, 348], [599, 456, 669, 421], [669, 421, 611, 367], \n",
    "                                [784, 312, 762, 228], [879, 350, 962, 280], [886, 438, 982, 471], [846, 506, 933, 563], [716, 549, 739, 645], [717, 550, 640, 611]])\n",
    "        \n",
    "        self.barrier_array = np.array([[203, 270, 291, 151], [291, 151, 485, 181], [485, 181, 535, 349], [535, 349, 612, 367], [612, 367, 762, 226], \n",
    "                                        [762, 226, 962, 280], [962, 280, 980, 469], [980, 469, 934, 564], [934, 564, 739, 647], [739, 647, 641, 612],\n",
    "                                        [641, 612, 598, 455], [598, 455, 515, 432], [515, 432, 403, 528], [403, 528, 240, 501], [240, 501, 186, 390], \n",
    "                                        [186, 390, 205, 264], [327, 238, 279, 335], [279, 335, 297, 405], [297, 405, 391, 424], [391, 424, 449, 368], \n",
    "                                        [449, 368, 428, 274], [428, 274, 326, 239], [670, 421, 716, 550], [716, 550, 848, 504], [848, 504, 885, 438], \n",
    "                                        [885, 438, 879, 350], [879, 350, 784, 312], [784, 312, 670, 421]])\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):   \n",
    "        done = False\n",
    "\n",
    "        self.reward = 0\n",
    "        self.num_steps += 1\n",
    "        if action==0:\n",
    "            self.car_angle += self.rotation_speed  \n",
    "        elif action==2:\n",
    "            self.car_angle -= self.rotation_speed \n",
    "\n",
    "        if len(self.reward_lines_array)>1:\n",
    "            for index, line in enumerate(self.reward_lines_array):\n",
    "\n",
    "                x1, y1, x2, y2 = line\n",
    "                if  ((intersection(x1, y1, x2, y2, self.car_x-10, self.car_y-10, self.car_x+10, self.car_y+10)) or \n",
    "                                                    (intersection(x1, y1, x2, y2, self.car_x-10, self.car_y+10, self.car_x+10, self.car_y-10))):\n",
    "                    if np.any(self.reward_last):\n",
    "                        if not np.array_equal(np.array(line), self.reward_last):\n",
    "                            self.reward = 1\n",
    "                            self.barrier_array = np.append(self.barrier_array, self.reward_lines_array[np.all(self.reward_lines_array == self.reward_last, axis=1)], axis=0)\n",
    "                            self.reward_lines_array = self.reward_lines_array[~np.all(self.reward_lines_array == self.reward_last, axis=1)]\n",
    "                    else:\n",
    "                        self.reward = 1\n",
    "                    self.reward_last = np.array(line)\n",
    "                    \n",
    "\n",
    "        else:\n",
    "            self.reward_lines_array = np.array([[428, 274, 486, 181], [328, 239, 290, 153], [278, 333, 203, 267], [280, 337, 186, 391], [296, 406, 239, 500], \n",
    "                                            [390, 426, 402, 527], [449, 367, 517, 433], [449, 368, 537, 348], [599, 456, 669, 421], [669, 421, 611, 367], \n",
    "                                            [784, 312, 762, 228], [879, 350, 962, 280], [886, 438, 982, 471], [846, 506, 933, 563], [716, 549, 739, 645], [717, 550, 640, 611]])\n",
    "            \n",
    "            self.barrier_array = np.array([[203, 270, 291, 151], [291, 151, 485, 181], [485, 181, 535, 349], [535, 349, 612, 367], [612, 367, 762, 226], \n",
    "                                            [762, 226, 962, 280], [962, 280, 980, 469], [980, 469, 934, 564], [934, 564, 739, 647], [739, 647, 641, 612],\n",
    "                                            [641, 612, 598, 455], [598, 455, 515, 432], [515, 432, 403, 528], [403, 528, 240, 501], [240, 501, 186, 390], \n",
    "                                            [186, 390, 205, 264], [327, 238, 279, 335], [279, 335, 297, 405], [297, 405, 391, 424], [391, 424, 449, 368], \n",
    "                                            [449, 368, 428, 274], [428, 274, 326, 239], [670, 421, 716, 550], [716, 550, 848, 504], [848, 504, 885, 438], \n",
    "                                            [885, 438, 879, 350], [879, 350, 784, 312], [784, 312, 670, 421]])\n",
    "\n",
    "        for line in self.barrier_array:\n",
    "            x1, y1, x2, y2 = line\n",
    "            if (intersection(x1, y1, x2, y2, self.car_x-10, self.car_y-10, self.car_x+10, self.car_y+10)) or (intersection(x1, y1, x2, y2, self.car_x-10, self.car_y+10, self.car_x+10, self.car_y-10)):\n",
    "                self.reward += -5\n",
    "                done = True\n",
    "\n",
    "        radians = math.radians(self.car_angle)\n",
    "        position = Vector2(self.car_x, self.car_y)\n",
    "        direction = Vector2(math.cos(radians), -math.sin(radians))\n",
    "        position += direction * self.speed\n",
    "        self.car_x, self.car_y = position.x, position.y\n",
    "\n",
    "        end_xy = np.zeros((self.num_rays, 2))\n",
    "\n",
    "\n",
    "        for i in range(self.num_rays):\n",
    "            for j in range(2):\n",
    "                if j==1:\n",
    "                    end_xy[i][j] = self.car_y - self.ray_length * math.sin(math.radians( self.car_angle+i*360/ self.num_rays))\n",
    "                else:\n",
    "                    end_xy[i][j] = self.car_x + self.ray_length * math.cos(math.radians( self.car_angle+i*360/ self.num_rays))\n",
    "\n",
    "        self.line_intersection_xy = []\n",
    "        for end_x,end_y in end_xy:\n",
    "            self.line_intersection_xy.append(find_intersection_points( self.barrier_array, self.car_x, self.car_y, end_x, end_y))\n",
    "\n",
    "        distance_line_array = []\n",
    "        for i in range(len(self.line_intersection_xy)):\n",
    "            distance, closest_intersection = find_distance_to_intersection(self.line_intersection_xy[i], self.car_x, self.car_y)\n",
    "            if closest_intersection is not None and not math.isinf(distance):\n",
    "                distance_line_array.append(distance)\n",
    "            else:\n",
    "                distance_line_array.append(1000)\n",
    "                # self.reward = -10\n",
    "                # done = True\n",
    "        \n",
    "        # self.car_xy = np.array([int(self.car_x), int(self.car_y)])\n",
    "\n",
    "        # self.state = np.concatenate([self.car_xy, distance_line_array])\n",
    "        distance_line_array = np.array(distance_line_array)\n",
    "        self.distance_array_for_fit = np.vstack((distance_line_array, self.distance_array_for_fit))[:500]\n",
    "   \n",
    "\n",
    "        scaler.fit(self.distance_array_for_fit)\n",
    "        scaled_distance_line_array = scaler.transform(distance_line_array.reshape(1, -1))\n",
    "\n",
    "        self.state = scaled_distance_line_array.flatten()\n",
    "        \n",
    "        return self.state, self.reward, done\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "        pygame.draw.circle(self.car_surface, self.car_color, (20,20), self.car_width)\n",
    "\n",
    "        clock = pygame.time.Clock()\n",
    "        self.screen.fill(self.WHITE) \n",
    "\n",
    "        pygame.draw.line(self.screen, self.BLACK, (self.car_x-10, self.car_y-10), (self.car_x+10, self.car_y+10), 5)\n",
    "        pygame.draw.line(self.screen, self.BLACK, (self.car_x-10, self.car_y+10), (self.car_x+10, self.car_y-10), 5)\n",
    "        for line in self.barrier_array:\n",
    "            x1, y1, x2, y2 = line\n",
    "            pygame.draw.line(self.screen, self.BLACK, (x1, y1), (x2, y2))\n",
    "\n",
    "\n",
    "        for line in self.reward_lines_array:\n",
    "            x1, y1, x2, y2 = line\n",
    "            pygame.draw.line(self.screen, self.GREEN, (x1, y1), (x2, y2))\n",
    "\n",
    "        pygame.draw.circle(self.screen, self.BLACK, (self.car_x, self.car_y), (10))\n",
    "\n",
    "        for i, intersection_points in enumerate(self.line_intersection_xy):\n",
    "            closest_distance, closest_intersection = find_distance_to_intersection(intersection_points, self.car_x, self.car_y)\n",
    "            if closest_intersection:\n",
    "                if i == 0:\n",
    "                    pygame.draw.line(self.screen, self.BLACK, (self.car_x, self.car_y), (int(closest_intersection[0]), int(closest_intersection[1])), 5)\n",
    "                else:\n",
    "                    pygame.draw.line(self.screen, self.BLACK, (self.car_x, self.car_y), (int(closest_intersection[0]), int(closest_intersection[1])), 1)\n",
    "                pygame.draw.circle(self.screen, self.RED, (int(closest_intersection[0]), int(closest_intersection[1])), 5)\n",
    "        pygame.display.flip()\n",
    "\n",
    "\n",
    "        keys = pygame.key.get_pressed()\n",
    "        if keys[pygame.K_LEFT]:\n",
    "            self.car_angle += self.rotation_speed  \n",
    "        if keys[pygame.K_RIGHT]:\n",
    "            self.car_angle -= self.rotation_speed  \n",
    "        if keys[pygame.K_ESCAPE]:\n",
    "            pygame.quit()\n",
    "        \n",
    "        clock.tick(120)\n",
    "\n",
    "env = MyCarEnv()\n",
    "\n",
    "\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, output_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1).to(device)\n",
    "        # self.fc2 = nn.Linear(hidden_size1, hidden_size2).to(device)\n",
    "        # self.fc3 = nn.Linear(hidden_size2, hidden_size3).to(device)\n",
    "        self.fc4 = nn.Linear(hidden_size1, output_size).to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x)).to(device)\n",
    "        x = torch.softmax(self.fc4(x), dim=-1).to(device)\n",
    "        return x\n",
    "\n",
    "class PolicyNetwork1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(PolicyNetwork1, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1).to(device)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2).to(device)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size).to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x)).to(device)\n",
    "        x = torch.relu(self.fc2(x)).to(device)\n",
    "        x = torch.softmax(self.fc3(x), dim=-1).to(device)\n",
    "        return x\n",
    "    \n",
    "class PolicyNetwork2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
    "        super(PolicyNetwork2, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1).to(device)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2).to(device)\n",
    "        self.fc3 = nn.Linear(hidden_size2, hidden_size3).to(device)\n",
    "        self.fc4 = nn.Linear(hidden_size3, output_size).to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x)).to(device)\n",
    "        x = torch.relu(self.fc2(x)).to(device)\n",
    "        x = torch.relu(self.fc3(x)).to(device)\n",
    "        x = torch.softmax(self.fc4(x), dim=-1).to(device)\n",
    "        return x\n",
    "\n",
    "\n",
    "def select_action(policy_net, state):\n",
    "    state = torch.tensor(state, dtype=torch.float32)\n",
    "    action_probs = policy_net(state)\n",
    "    action = np.random.choice(len(action_probs.detach().numpy()), p=action_probs.detach().numpy())\n",
    "    return action\n",
    "\n",
    "\n",
    "def train(policy_net, optimizer, episodes, max_reward, hidden_size1, hidden_size2, hidden_size3, learning_rate):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_states = []\n",
    "        episode_actions = []\n",
    "        episode_rewards = []\n",
    "        steps = 0\n",
    "        while True:\n",
    "            action = select_action(policy_net, state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            steps += 1\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            episode_states.append(state)\n",
    "            episode_actions.append(action)\n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "           \n",
    "        \n",
    "            state = next_state\n",
    "            if steps >= 1000000:\n",
    "                torch.save(policy_net, f'net/tarakan1_{max_reward+5}_{hidden_size1}_{hidden_size2}_{hidden_size3}_{learning_rate}.pth')\n",
    "                print(f\"Tarakan1 Ultra Episodes {episode}:{hidden_size1}_{hidden_size2}_{hidden_size3}_{learning_rate}  SAVED!\")\n",
    "                done = True\n",
    "            if done:\n",
    "                break\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for r in episode_rewards[::-1]:\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0, R)\n",
    "\n",
    "        policy_loss = []\n",
    "        for i in range(len(episode_states)):\n",
    "            action = episode_actions[i]\n",
    "            G = returns[i]\n",
    "            state = episode_states[i]\n",
    "            action_prob = policy_net(torch.tensor(state, dtype=torch.float32))[action]\n",
    "            policy_loss.append(-torch.log(action_prob) * G)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss = torch.stack(policy_loss).sum()\n",
    "\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        if sum(episode_rewards) > max_reward:\n",
    "            max_reward = sum(episode_rewards)\n",
    "            torch.save(policy_net, f'net/tarakan1_{max_reward+5}_{hidden_size1}_{hidden_size2}_{hidden_size3}_{learning_rate}.pth')\n",
    "            print(f\"Episode {episode}: Max Reward = {max_reward+5} SAVED!\")\n",
    "        print(f\"Episode {episode}: Total Reward = {sum(episode_rewards)+5} Max Reward = {max_reward+5}, hidden_size1 = {hidden_size1}, hidden_size2 = {hidden_size2}, hidden_size3 = {hidden_size3}, леарнинг рейт = {learning_rate}\")\n",
    "    return max_reward\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test(policy_net, num_episodes=10):\n",
    "    total_reward = -10\n",
    "    for episode in range(num_episodes):\n",
    "        \n",
    "        state = env.reset()\n",
    "\n",
    "        while True:\n",
    "            action = select_action(policy_net, state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            if done:\n",
    "                print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "    env.close()\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "render = False\n",
    "\n",
    "if render:\n",
    "    env.screen = pygame.display.set_mode((env.WINDOW_WIDTH, env.WINDOW_HEIGHT))\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "episodes = 1000\n",
    "\n",
    "\n",
    "results = []\n",
    "max_reward = -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(22, 100):\n",
    "    input_size = env.num_rays\n",
    "    output_size = 3\n",
    "    hidden_size1 = i\n",
    "    learning_rate = 0.01\n",
    "\n",
    "    policy_net = PolicyNetwork(input_size, hidden_size1, output_size).to(device)\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "    max_reward = train(policy_net, optimizer, episodes, max_reward, hidden_size1, 0, 0, learning_rate)\n",
    "\n",
    "    restest = (f'1 скрытый слой, hidden_size1 = {hidden_size1}, леарнинг рейт = {learning_rate}, счет = {test(policy_net, num_episodes=10)}')\n",
    "    print(restest)\n",
    "    results.append(restest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes=10000\n",
    "for i in range(1, 100):\n",
    "    input_size = env.num_rays\n",
    "    output_size = 3\n",
    "    hidden_size1 = i\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    policy_net = PolicyNetwork(input_size, hidden_size1, output_size).to(device)\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "    max_reward = train(policy_net, optimizer, episodes, max_reward, hidden_size1, 0, 0, learning_rate)\n",
    "\n",
    "    restest = (f'1 скрытый слой, hidden_size1 = {hidden_size1}, леарнинг рейт = {learning_rate}, счет = {test(policy_net, num_episodes=10)}')\n",
    "    print(restest)\n",
    "    results.append(restest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 100):\n",
    "    for a in range(1, 100):\n",
    "        input_size = env.num_rays\n",
    "        output_size = 3\n",
    "        hidden_size1 = i\n",
    "        hidden_size2 = a\n",
    "        learning_rate = 0.001\n",
    "        \n",
    "        policy_net = PolicyNetwork1(input_size, hidden_size1, hidden_size2, output_size).to(device)\n",
    "        optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "\n",
    "        max_reward = train(policy_net, optimizer, episodes, max_reward, hidden_size1, hidden_size2, 0, learning_rate)\n",
    "        restest = (f'2 скрытых слоя, hidden_size1 = {hidden_size1}, hidden_size2 = {hidden_size2},леарнинг рейт = {learning_rate}, счет = {test(policy_net, num_episodes=10)}')\n",
    "        print(restest)\n",
    "        results.append(restest)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_size = env.num_rays\n",
    "output_size = 3\n",
    "hidden_size1 = 35\n",
    "learning_rate = 0.01\n",
    "\n",
    "policy_net = PolicyNetwork(input_size, hidden_size1, output_size).to(device)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "max_reward = train(policy_net, optimizer, episodes, max_reward, hidden_size1, 0, 0, learning_rate)\n",
    "\n",
    "restest = (f'1 скрытый слой, hidden_size1 = {hidden_size1}, леарнинг рейт = {learning_rate}, счет = {test(policy_net, num_episodes=10)}')\n",
    "print(restest)\n",
    "results.append(restest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = -14\n",
      "Episode 2: Total Reward = -17\n",
      "Episode 3: Total Reward = -16\n",
      "Episode 4: Total Reward = -19\n",
      "Episode 5: Total Reward = -21\n",
      "Episode 6: Total Reward = -20\n",
      "Episode 7: Total Reward = -18\n",
      "Episode 8: Total Reward = -15\n",
      "Episode 9: Total Reward = -3\n",
      "Episode 10: Total Reward = 7\n",
      "Episode 11: Total Reward = 9\n",
      "Episode 12: Total Reward = 10\n",
      "Episode 13: Total Reward = 10\n",
      "Episode 14: Total Reward = 12\n",
      "Episode 15: Total Reward = 14\n",
      "Episode 16: Total Reward = 41\n",
      "Episode 17: Total Reward = 76\n",
      "Episode 18: Total Reward = 118\n",
      "Episode 19: Total Reward = 122\n",
      "Episode 20: Total Reward = 145\n",
      "Episode 21: Total Reward = 187\n",
      "Episode 22: Total Reward = 207\n",
      "Episode 23: Total Reward = 219\n",
      "Episode 24: Total Reward = 222\n",
      "Episode 25: Total Reward = 344\n",
      "Episode 26: Total Reward = 348\n",
      "Episode 27: Total Reward = 351\n",
      "Episode 28: Total Reward = 393\n",
      "Episode 29: Total Reward = 397\n",
      "Episode 30: Total Reward = 401\n",
      "Episode 31: Total Reward = 412\n",
      "Episode 32: Total Reward = 422\n",
      "Episode 33: Total Reward = 586\n",
      "Episode 34: Total Reward = 590\n",
      "Episode 35: Total Reward = 610\n",
      "Episode 36: Total Reward = 614\n",
      "Episode 37: Total Reward = 619\n",
      "Episode 38: Total Reward = 644\n",
      "Episode 39: Total Reward = 670\n",
      "Episode 40: Total Reward = 671\n",
      "Episode 41: Total Reward = 668\n",
      "Episode 42: Total Reward = 669\n",
      "Episode 43: Total Reward = 673\n",
      "Episode 44: Total Reward = 669\n",
      "Episode 45: Total Reward = 673\n",
      "Episode 46: Total Reward = 677\n",
      "Episode 47: Total Reward = 701\n",
      "Episode 48: Total Reward = 705\n",
      "Episode 49: Total Reward = 763\n",
      "Episode 50: Total Reward = 789\n",
      "Episode 51: Total Reward = 799\n",
      "Episode 52: Total Reward = 803\n",
      "Episode 53: Total Reward = 861\n",
      "Episode 54: Total Reward = 929\n",
      "Episode 55: Total Reward = 932\n",
      "Episode 56: Total Reward = 1065\n",
      "Episode 57: Total Reward = 1070\n",
      "Episode 58: Total Reward = 1128\n",
      "Episode 59: Total Reward = 1148\n",
      "Episode 60: Total Reward = 1152\n",
      "Episode 61: Total Reward = 1194\n",
      "Episode 62: Total Reward = 1245\n",
      "Episode 63: Total Reward = 1361\n",
      "Episode 64: Total Reward = 1461\n",
      "Episode 65: Total Reward = 1471\n",
      "Episode 66: Total Reward = 1475\n",
      "Episode 67: Total Reward = 1519\n",
      "Episode 68: Total Reward = 1573\n",
      "Episode 69: Total Reward = 1606\n",
      "Episode 70: Total Reward = 1712\n",
      "Episode 71: Total Reward = 1739\n",
      "Episode 72: Total Reward = 1765\n",
      "Episode 73: Total Reward = 1785\n",
      "Episode 74: Total Reward = 1805\n",
      "Episode 75: Total Reward = 1840\n",
      "Episode 76: Total Reward = 1844\n",
      "Episode 77: Total Reward = 1845\n",
      "Episode 78: Total Reward = 1872\n",
      "Episode 79: Total Reward = 1915\n",
      "Episode 80: Total Reward = 1925\n",
      "Episode 81: Total Reward = 1936\n",
      "Episode 82: Total Reward = 1978\n",
      "Episode 83: Total Reward = 1983\n",
      "Episode 84: Total Reward = 2009\n",
      "Episode 85: Total Reward = 2035\n",
      "Episode 86: Total Reward = 2077\n",
      "Episode 87: Total Reward = 2097\n",
      "Episode 88: Total Reward = 2139\n",
      "Episode 89: Total Reward = 2325\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "video system not initialized",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m env\u001b[38;5;241m.\u001b[39mscreen \u001b[38;5;241m=\u001b[39m pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mset_mode((env\u001b[38;5;241m.\u001b[39mWINDOW_WIDTH, env\u001b[38;5;241m.\u001b[39mWINDOW_HEIGHT))\n\u001b[0;32m      3\u001b[0m policy_net \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnet/tarakan1/6_0_0_0.0001_442.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[1], line 387\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(policy_net, num_episodes)\u001b[0m\n\u001b[0;32m    384\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m render:\n\u001b[1;32m--> 387\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Total Reward = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 223\u001b[0m, in \u001b[0;36mMyCarEnv.render\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 223\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m event\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m pygame\u001b[38;5;241m.\u001b[39mQUIT:\n\u001b[0;32m    225\u001b[0m             running \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31merror\u001b[0m: video system not initialized"
     ]
    }
   ],
   "source": [
    "render = True\n",
    "env.screen = pygame.display.set_mode((env.WINDOW_WIDTH, env.WINDOW_HEIGHT))\n",
    "policy_net = torch.load(f'net/tarakan1/6_0_0_0.0001_442.pth')\n",
    "print(test(policy_net, num_episodes=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net, f'net/tarakan_ultra.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
